{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1d5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Environment\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69df7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5993b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_states():\n",
    "    \n",
    "    states = []\n",
    "    \n",
    "    for i in range(1,5):\n",
    "        for j in range(1,9):\n",
    "            \n",
    "            if (i,j) != (2,1) and (i,j) != (2,7):\n",
    "                states.append(((i,j), False))\n",
    "            \n",
    "            if (i,j) != (1,7):\n",
    "                states.append(((i,j), True))\n",
    "            \n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcbb278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon, q):\n",
    "        \n",
    "    pos = state[0]\n",
    "    tool = state[1]\n",
    "\n",
    "    # with probability 1-epsilon the action maximizing Q(current_state, action) is chosen; \n",
    "    #      this option is coded with 0\n",
    "    # with prob epsilon, a random action (among the legal ones) is chosen; this option is coded with 1\n",
    "\n",
    "    choice = np.random.choice([0,1], p=[1-epsilon, epsilon])\n",
    "\n",
    "    # if choice == 0, then the action maximizing Q(s,a) must be chosen\n",
    "    if choice == 0:\n",
    "\n",
    "        # list used in case of ties (2 or more actions such that Q(s,a) = Q(s,a'))\n",
    "        possible_actions = []\n",
    "\n",
    "        q_val = float('-inf')\n",
    "\n",
    "        for action in e.get_legal_actions(pos, tool):\n",
    "            if q[(state, action)] >= q_val:\n",
    "                q_val = q[(state, action)]\n",
    "                possible_actions.append((action, q_val))\n",
    "\n",
    "        possible_actions = sorted(possible_actions, key=lambda x: x[1])\n",
    "\n",
    "        chosen_action = possible_actions[len(possible_actions)-1][0]\n",
    "\n",
    "    # otherwise, a random action is chosen\n",
    "    else:\n",
    "        chosen_action = random.choice(e.get_legal_actions(pos, tool))\n",
    "        \n",
    "    return chosen_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40bae226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(epsilon, q):\n",
    "    \n",
    "    # reset the environment\n",
    "    e.reset()\n",
    "    \n",
    "    # episode is a list of triples (state, action, reward), returned by the function\n",
    "    episode = []\n",
    "    \n",
    "    # retrieve the current state\n",
    "    current_state = e.get_curr_state()\n",
    "    \n",
    "    # choice the action, following the epsilon-greedy policy\n",
    "    action = epsilon_greedy_policy(current_state, epsilon, q)\n",
    "    \n",
    "    # if the returned action is None or the lenght of the episode >= 33, then the episode ended\n",
    "    while action is not None and len(episode) <= 10000:\n",
    "        \n",
    "        # apply the selected action, get the reward and the new state\n",
    "        new_state, reward = e.apply(action)\n",
    "        \n",
    "        # append the newly discovered triple to the episode list\n",
    "        episode.append((current_state, action, reward))\n",
    "        \n",
    "        # update current state\n",
    "        current_state = new_state\n",
    "        \n",
    "        if e.position == (1,7) and e.tool:\n",
    "            action = None\n",
    "        else:\n",
    "            # get the next action, for the new current state\n",
    "            action = epsilon_greedy_policy(current_state, epsilon, q)\n",
    "    \n",
    "    episode.append(((e.position, e.tool), None, None))\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d19898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_return(episode, j):\n",
    "    \n",
    "    # j is the index of tuple for which this function is called\n",
    "    gamma = e.GAMMA\n",
    "    \n",
    "    cumulative_rew = 0\n",
    "    \n",
    "    # len(episode)-1 since the last triple is only made of a state, action and return are None\n",
    "    for i in range(j, len(episode)-1):\n",
    "        \n",
    "        # retrieve the reward value from the current triple (state, action, reward), starting from the j-th triple\n",
    "        rew = episode[i][2]\n",
    "        \n",
    "        # sum the current (discounted) reward to the cumulative reward \n",
    "        cumulative_rew += rew * (gamma**(i-j))\n",
    "            \n",
    "    return cumulative_rew\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d5d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(k):\n",
    "    \n",
    "    # epsilon initialized to 1\n",
    "    epsilon = 1\n",
    "    \n",
    "    # dictionary (state, action) -> G value\n",
    "    g = {}\n",
    "    \n",
    "    # dictionary (state, action) -> N value\n",
    "    n = {}\n",
    "    \n",
    "    # dictionary (state, action) -> Q value\n",
    "    q = {}\n",
    "    \n",
    "    # initialize G, N and Q values to zero\n",
    "    for state in generate_all_states():\n",
    "        \n",
    "        for action in e.get_legal_actions(state[0], state[1]):\n",
    "            g[(state, action)] = 0\n",
    "            n[(state, action)] = 0\n",
    "            q[(state, action)] = 0\n",
    "    \n",
    "    # create initial epsilon-greedy policy\n",
    "    policy = {}\n",
    "    \n",
    "    for i in range(1, k+1):\n",
    "        \n",
    "        episode = sample_episode(epsilon, q)\n",
    "        \n",
    "        # scan the episode and compute G, N and Q values\n",
    "        for j in range(0, len(episode)):\n",
    "                \n",
    "            state = episode[j][0]\n",
    "            action = episode[j][1]\n",
    "            reward = episode[j][2]\n",
    "\n",
    "            # check needed because the last state in an episode has no action and reward\n",
    "            if action is not None and reward is not None:\n",
    "                n[(state, action)] += 1\n",
    "                g[(state, action)] += compute_return(episode, j)\n",
    "                q[(state, action)] = g[(state, action)]/n[(state, action)]\n",
    "        \n",
    "        \n",
    "        # update epsilon\n",
    "        epsilon = 1/(i+1)\n",
    "        \n",
    "    # build policy from sampled episodes\n",
    "    for state in generate_all_states():\n",
    "        \n",
    "        if state != ((1,7), True):\n",
    "        \n",
    "            pos = state[0]\n",
    "            tool = state[1]\n",
    "\n",
    "            q_val = float('-inf')\n",
    "            \n",
    "            possible_actions = []\n",
    "\n",
    "            for action in e.get_legal_actions(pos, tool):\n",
    "                if q[(state, action)] >= q_val:\n",
    "                    q_val = q[(state, action)]\n",
    "                    possible_actions.append((action, q_val))\n",
    "\n",
    "            possible_actions = sorted(possible_actions, key=lambda x: x[1])\n",
    "\n",
    "            policy[state] = possible_actions[len(possible_actions)-1][0]\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb69a61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_policy = policy_improvement(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9d8a32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((1, 1), False): 'right',\n",
       " ((1, 1), True): 'right',\n",
       " ((1, 2), False): 'right',\n",
       " ((1, 2), True): 'left',\n",
       " ((1, 3), False): 'right',\n",
       " ((1, 3), True): 'down',\n",
       " ((1, 4), False): 'down',\n",
       " ((1, 4), True): 'down',\n",
       " ((1, 5), False): 'right',\n",
       " ((1, 5), True): 'right',\n",
       " ((1, 6), False): 'down',\n",
       " ((1, 6), True): 'right',\n",
       " ((1, 7), False): 'left',\n",
       " ((1, 8), False): 'left',\n",
       " ((1, 8), True): 'left',\n",
       " ((2, 1), True): 'down',\n",
       " ((2, 2), False): 'right',\n",
       " ((2, 2), True): 'up',\n",
       " ((2, 3), False): 'right',\n",
       " ((2, 3), True): 'right',\n",
       " ((2, 4), False): 'down',\n",
       " ((2, 4), True): 'down',\n",
       " ((2, 5), False): 'right',\n",
       " ((2, 5), True): 'up',\n",
       " ((2, 6), False): 'down',\n",
       " ((2, 6), True): 'up',\n",
       " ((2, 7), True): 'right',\n",
       " ((2, 8), False): 'left',\n",
       " ((2, 8), True): 'down',\n",
       " ((3, 1), False): 'up',\n",
       " ((3, 1), True): 'right',\n",
       " ((3, 2), False): 'right',\n",
       " ((3, 2), True): 'right',\n",
       " ((3, 3), False): 'right',\n",
       " ((3, 3), True): 'right',\n",
       " ((3, 4), False): 'right',\n",
       " ((3, 4), True): 'right',\n",
       " ((3, 5), False): 'right',\n",
       " ((3, 5), True): 'up',\n",
       " ((3, 6), False): 'down',\n",
       " ((3, 6), True): 'up',\n",
       " ((3, 7), False): 'up',\n",
       " ((3, 7), True): 'right',\n",
       " ((3, 8), False): 'left',\n",
       " ((3, 8), True): 'down',\n",
       " ((4, 1), False): 'right',\n",
       " ((4, 1), True): 'right',\n",
       " ((4, 2), False): 'right',\n",
       " ((4, 2), True): 'left',\n",
       " ((4, 3), False): 'right',\n",
       " ((4, 3), True): 'right',\n",
       " ((4, 4), False): 'up',\n",
       " ((4, 4), True): 'left',\n",
       " ((4, 5), False): 'right',\n",
       " ((4, 5), True): 'right',\n",
       " ((4, 6), False): 'right',\n",
       " ((4, 6), True): 'up',\n",
       " ((4, 7), False): 'right',\n",
       " ((4, 7), True): 'left',\n",
       " ((4, 8), False): 'up',\n",
       " ((4, 8), True): 'left'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f977918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
